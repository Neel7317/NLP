{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLFdRwjWYtlLmX7gbWHwDx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neel7317/NLP/blob/main/Sequencial_chain_using_Langchain(LLMs).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMjNSjIzunFm",
        "outputId": "2527b8a5-8923-4638-c3e5-cb61eb3bc8ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting huggingface_hub\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (23.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.4)\n",
            "Installing collected packages: huggingface_hub\n",
            "Successfully installed huggingface_hub-0.16.4\n"
          ]
        }
      ],
      "source": [
        "pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install wikipedia==1.4.0 google-search-results==2.4.2 better-profanity==0.7.0 sqlalchemy==2.0.15"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKf2qJBxuq9U",
        "outputId": "001cc301-9aac-4447-e018-3cd405d04900"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia==1.4.0\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting google-search-results==2.4.2\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting better-profanity==0.7.0\n",
            "  Downloading better_profanity-0.7.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sqlalchemy==2.0.15\n",
            "  Downloading SQLAlchemy-2.0.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia==1.4.0) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia==1.4.0) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy==2.0.15) (4.7.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy==2.0.15) (2.0.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia==1.4.0) (3.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia==1.4.0) (2.4.1)\n",
            "Building wheels for collected packages: wikipedia, google-search-results\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11680 sha256=293b1d1976e51b2c7da2d8303ec54ca87a5af21b3977d90b8a56442e5008d1ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32001 sha256=94838b144cca4bce0342cccebb9b4725578fb8af1999d3ed4de4648b7c2ffa72\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built wikipedia google-search-results\n",
            "Installing collected packages: sqlalchemy, better-profanity, wikipedia, google-search-results\n",
            "  Attempting uninstall: sqlalchemy\n",
            "    Found existing installation: SQLAlchemy 2.0.19\n",
            "    Uninstalling SQLAlchemy-2.0.19:\n",
            "      Successfully uninstalled SQLAlchemy-2.0.19\n",
            "Successfully installed better-profanity-0.7.0 google-search-results-2.4.2 sqlalchemy-2.0.15 wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "os.environ[\"SERPAPI_API_KEY\"] = \"cda845f23573e31b819b441899e419b21326a956e07a63d6a228dd78b1740203\""
      ],
      "metadata": {
        "id": "0YcyjRTBuw7k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `JekyllHyde` - A self moderating system for social media\n",
        "\n",
        "In this section we will build an AI system that consists of two LLMs. `Jekyll` will be an LLM designed to read in a social media post and create a new comment. However, `Jekyll` can be moody at times so there will always be a chance that it creates a negative-sentiment comment... we need to make sure we filter those out. Luckily, that is the role of `Hyde`, the other LLM that will watch what `Jekyll` says and flag any negative comments to be removed."
      ],
      "metadata": {
        "id": "mt7Fk-5NxmKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StQB-4L7u0UN",
        "outputId": "e7db2690-96ce-42b4-af47-681989c6958b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.0.247-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.15)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.5.13-py3-none-any.whl (26 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.11 (from langchain)\n",
            "  Downloading langsmith-0.0.15-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Collecting openapi-schema-pydantic<2.0,>=1.2 (from langchain)\n",
            "  Downloading openapi_schema_pydantic-1.2.4-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, typing-inspect, openapi-schema-pydantic, langsmith, dataclasses-json, langchain\n",
            "Successfully installed dataclasses-json-0.5.13 langchain-0.0.247 langsmith-0.0.15 marshmallow-3.20.1 mypy-extensions-1.0.0 openapi-schema-pydantic-1.2.4 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub\n",
        "from langchain import PromptTemplate, LLMChain"
      ],
      "metadata": {
        "id": "K9juMC6Gu5-n"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Step 1 - Letting Jekyll Speak\n",
        "# Building the Jekyll Prompt\n"
      ],
      "metadata": {
        "id": "BP0kxXLMxu3U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate\n",
        "import numpy as np\n",
        "\n",
        "# Our template for Jekyll will instruct it on how it should respond, and what variables (using the {text} syntax) it should use.\n",
        "jekyll_template = \"\"\"\n",
        "You are a social media post commenter, you will respond to the following post with a {sentiment} response.\n",
        "Post:\" {social_post}\"\n",
        "Comment:\n",
        "\"\"\"\n",
        "# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\n",
        "jekyll_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"sentiment\", \"social_post\"],\n",
        "    template=jekyll_template,\n",
        ")\n",
        "\n",
        "# Okay now that's ready we need to make the randomized sentiment\n",
        "random_sentiment = \"nice\"\n",
        "if np.random.rand() < 0.3:\n",
        "    random_sentiment = \"mean\"\n",
        "# We'll also need our social media post:\n",
        "social_post = \"I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
        "\n",
        "# Let's create the prompt and print it out, this will be given to the LLM.\n",
        "jekyll_prompt = jekyll_prompt_template.format(\n",
        "    sentiment=random_sentiment, social_post=social_post\n",
        ")\n",
        "print(f\"Jekyll prompt:{jekyll_prompt}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTFkg_fuvCsQ",
        "outputId": "a21302af-35e4-4ffe-addf-bb78ceb1c2f1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jekyll prompt:\n",
            "You are a social media post commenter, you will respond to the following post with a nice response. \n",
            "Post:\" I can't believe I'm learning about LangChain in this MOOC, there is so much to learn and so far the instructors have been so helpful. I'm having a lot of fun learning! #AI #Databricks\"\n",
            "Comment: \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2 - Giving Jekyll a brain!\n",
        "# Building the Jekyll LLM\n",
        "# Note: provide an option for you to use either Hugging Face or OpenAI."
      ],
      "metadata": {
        "id": "aXsiyYahx1kI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3 - What does Jekyll Say?\n",
        "# Building our Prompt-LLM Chain"
      ],
      "metadata": {
        "id": "AK3J4i03yBhq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import chains\n",
        "repo_id = \"google/flan-t5-xxl\"\n",
        "\n",
        "jekyll_llm = HuggingFaceHub(\n",
        "    repo_id=repo_id\n",
        ")\n",
        "\n",
        "from langchain.chains import LLMChain\n",
        "from better_profanity import profanity\n",
        "\n",
        "\n",
        "jekyll_chain = LLMChain(\n",
        "    llm=jekyll_llm,\n",
        "    prompt=jekyll_prompt_template,\n",
        "    output_key=\"jekyll_said\",\n",
        "    verbose=False,\n",
        ")  # Now that we've chained the LLM and prompt, the output of the formatted prompt will pass directly to the LLM.\n",
        "\n",
        "# To run our chain we use the .run() command and input our variables as a dict\n",
        "jekyll_said = jekyll_chain.run(\n",
        "    {\"sentiment\": random_sentiment, \"social_post\": social_post}\n",
        ")\n",
        "\n",
        "# Before printing what Jekyll said, let's clean it up:\n",
        "cleaned_jekyll_said = profanity.censor(jekyll_said)\n",
        "print(f\"Jekyll said:{cleaned_jekyll_said}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-x8MEIWEvQ5V",
        "outputId": "24aa039a-8c23-482e-df0e-a0c7481b3ff0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jekyll said:I'm learning about LangChain in this MOOC, there is so much to learn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 - Time for Jekyll to Hyde"
      ],
      "metadata": {
        "id": "aR7OGLspyD8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 We will build the prompt template\n",
        "# Our template for Hyde will take Jekyll's comment and do some sentiment analysis.\n",
        "hyde_template = \"\"\"\n",
        "You are Hyde, the moderator of an online forum, you are strict and will not tolerate any negative comments. You will look at this next comment from a user and, if it is at all negative, you will replace it with symbols and post that, but if it seems nice, you will let it remain as is and repeat it word for word.\n",
        "Original comment: {jekyll_said}\n",
        "Edited comment:\n",
        "\"\"\"\n",
        "# We use the PromptTemplate class to create an instance of our template that will use the prompt from above and store variables we will need to input when we make the prompt.\n",
        "hyde_prompt_template = PromptTemplate(\n",
        "    input_variables=[\"jekyll_said\"],\n",
        "    template=hyde_template,\n",
        ")\n",
        "# -----------------------------------\n",
        "# -----------------------------------\n",
        "# 2 We connect an LLM for Hyde, (we could use a slightly more advanced model 'text-davinci-003 since we have some more logic in this prompt).\n",
        "\n",
        "hyde_llm = jekyll_llm\n",
        "# Uncomment the line below if you were to use OpenAI instead\n",
        "# hyde_llm = OpenAI(model=\"text-davinci-003\")\n",
        "\n",
        "# -----------------------------------\n",
        "# -----------------------------------\n",
        "# 3 We build the chain for Hyde\n",
        "hyde_chain = LLMChain(\n",
        "    llm=hyde_llm, prompt=hyde_prompt_template, verbose=False\n",
        ")  # Now that we've chained the LLM and prompt, the output of the formatted prompt will pass directly to the LLM.\n",
        "# -----------------------------------\n",
        "# -----------------------------------\n",
        "# 4 Let's run the chain with what Jekyll last said\n",
        "# To run our chain we use the .run() command and input our variables as a dict\n",
        "hyde_says = hyde_chain.run({\"jekyll_said\": jekyll_said})\n",
        "# Let's see what hyde said...\n",
        "print(f\"Hyde says: {hyde_says}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqDYOv_7woYD",
        "outputId": "0c1aba4f-2452-4385-c767-0428b2a89e4b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyde says: I'm learning about LangChain in this MOOC, there is so much to learn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5 - Creating `JekyllHyde`\n",
        "# Building our first Sequential Chain"
      ],
      "metadata": {
        "id": "15qgItEbyMfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import SequentialChain\n",
        "\n",
        "# The SequentialChain class takes in the chains we are linking together, as well as the input variables that will be added to the chain. These input variables can be used at any point in the chain, not just the start.\n",
        "jekyllhyde_chain = SequentialChain(\n",
        "    chains=[jekyll_chain, hyde_chain],\n",
        "    input_variables=[\"sentiment\", \"social_post\"],\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# We can now run the chain with our randomized sentiment, and the social post!\n",
        "jekyllhyde_chain.run({\"sentiment\": random_sentiment, \"social_post\": social_post})\n"
      ],
      "metadata": {
        "id": "1d7LXhs4xAYM",
        "outputId": "e5e2cc15-919f-4db3-ece4-bb0ad0114cdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm learning about LangChain in this MOOC, there is so much to learn\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hsRJDohCxISF"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}